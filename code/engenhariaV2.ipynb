{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One and only one of data and data_func must be set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Setup dos dados para treinamento usando PyCaret\u001b[39;00m\n\u001b[0;32m     51\u001b[0m setup_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshot_made_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     54\u001b[0m }\n\u001b[1;32m---> 56\u001b[0m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msetup_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Treinar modelos de classificação usando PyCaret\u001b[39;00m\n\u001b[0;32m     58\u001b[0m automl_model \u001b[38;5;241m=\u001b[39m automl(optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\ENGENHARIA_MACHINE_LEARNING_V2\\lib\\site-packages\\pycaret\\classification\\functional.py:595\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(data, data_func, target, index, train_size, test_data, ordinal_features, numeric_features, categorical_features, date_features, text_features, ignore_features, keep_features, preprocess, create_date_columns, imputation_type, numeric_imputation, categorical_imputation, iterative_imputation_iters, numeric_iterative_imputer, categorical_iterative_imputer, text_features_method, max_encoding_ohe, encoding_method, rare_to_value, rare_value, polynomial_features, polynomial_degree, low_variance_threshold, group_features, drop_groups, remove_multicollinearity, multicollinearity_threshold, bin_numeric_features, remove_outliers, outliers_method, outliers_threshold, fix_imbalance, fix_imbalance_method, transformation, transformation_method, normalize, normalize_method, pca, pca_method, pca_components, feature_selection, feature_selection_method, feature_selection_estimator, n_features_to_select, custom_pipeline, custom_pipeline_position, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, html, session_id, system_log, log_experiment, experiment_name, experiment_custom_tags, log_plots, log_profile, log_data, verbose, memory, profile, profile_kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m exp \u001b[38;5;241m=\u001b[39m _EXPERIMENT_CLASS()\n\u001b[0;32m    594\u001b[0m set_current_experiment(exp)\n\u001b[1;32m--> 595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mordinal_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordinal_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_date_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_date_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimputation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimputation_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_imputation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_imputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_imputation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_imputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterative_imputation_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterative_imputation_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_iterative_imputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_iterative_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_iterative_imputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_iterative_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_features_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_encoding_ohe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoding_ohe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrare_to_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrare_to_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrare_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrare_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolynomial_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolynomial_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_variance_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_variance_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_multicollinearity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_multicollinearity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulticollinearity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticollinearity_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbin_numeric_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_numeric_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_outliers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_outliers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutliers_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutliers_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutliers_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutliers_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_imbalance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imbalance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_imbalance_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imbalance_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_features_to_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_pipeline_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_stratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_stratify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhtml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_experiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_experiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_plots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_profile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_profile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\miniconda3\\envs\\ENGENHARIA_MACHINE_LEARNING_V2\\lib\\site-packages\\pycaret\\classification\\oop.py:699\u001b[0m, in \u001b[0;36mClassificationExperiment.setup\u001b[1;34m(self, data, data_func, target, index, train_size, test_data, ordinal_features, numeric_features, categorical_features, date_features, text_features, ignore_features, keep_features, preprocess, create_date_columns, imputation_type, numeric_imputation, categorical_imputation, iterative_imputation_iters, numeric_iterative_imputer, categorical_iterative_imputer, text_features_method, max_encoding_ohe, encoding_method, rare_to_value, rare_value, polynomial_features, polynomial_degree, low_variance_threshold, group_features, drop_groups, remove_multicollinearity, multicollinearity_threshold, bin_numeric_features, remove_outliers, outliers_method, outliers_threshold, fix_imbalance, fix_imbalance_method, transformation, transformation_method, normalize, normalize_method, pca, pca_method, pca_components, feature_selection, feature_selection_method, feature_selection_estimator, n_features_to_select, custom_pipeline, custom_pipeline_position, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, html, session_id, system_log, log_experiment, experiment_name, experiment_custom_tags, log_plots, log_profile, log_data, engine, verbose, memory, profile, profile_kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_setup_params(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    697\u001b[0m     data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    698\u001b[0m ):\n\u001b[1;32m--> 699\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne and only one of data and data_func must be set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    701\u001b[0m \u001b[38;5;66;03m# No extra code above this line\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Setup initialization ===================================== >>\u001b[39;00m\n\u001b[0;32m    704\u001b[0m runtime_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mValueError\u001b[0m: One and only one of data and data_func must be set"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.classification import *\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score\n",
    "\n",
    "# Carregar os datasets\n",
    "#C:\\Users\\Anderson\\Desktop\\REDES_NEURAIS\\ENGENHARIA_V2\\ENGENHARIA_MACHINE_LEARNING_INFNET\\data\\raw\\dataset_kobe_dev.parquet\n",
    "dev_dataset_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/raw/dataset_kobe_dev.parquet\"\n",
    "prod_dataset_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/raw/dataset_kobe_prod.parquet\"\n",
    "\n",
    "# Carregar os datasets de desenvolvimento e produção\n",
    "dev_df = pd.read_parquet(dev_dataset_path)\n",
    "prod_df = pd.read_parquet(prod_dataset_path)\n",
    "\n",
    "# Filtrar as colunas relevantes\n",
    "relevant_columns = ['lat', 'lon', 'minutes_remaining', 'period', 'playoffs', 'shot_distance', 'shot_made_flag']\n",
    "dev_df_filtered = dev_df[relevant_columns].copy()\n",
    "prod_df_filtered = prod_df[relevant_columns].copy()\n",
    "\n",
    "# Remover linhas com dados faltantes\n",
    "dev_df_filtered.dropna(inplace=True)\n",
    "prod_df_filtered.dropna(inplace=True)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino (80%) e teste (20%) usando uma escolha aleatória e estratificada\n",
    "dev_train, dev_test = train_test_split(dev_df_filtered, test_size=0.2, random_state=42, stratify=dev_df_filtered['shot_made_flag'])\n",
    "prod_train, prod_test = train_test_split(prod_df_filtered, test_size=0.2, random_state=42, stratify=prod_df_filtered['shot_made_flag'])\n",
    "    \n",
    "# Iniciar uma nova corrida no MLflow\n",
    "with mlflow.start_run(run_name=\"PipelineAplicacao\"):\n",
    "    # Calcular as proporções de treino e teste\n",
    "    dev_train_ratio = len(dev_train) / len(dev_df_filtered)\n",
    "    dev_test_ratio = len(dev_test) / len(dev_df_filtered)\n",
    "    prod_train_ratio = len(prod_train) / len(prod_df_filtered)\n",
    "    prod_test_ratio = len(prod_test) / len(prod_df_filtered)\n",
    "\n",
    "    # Registrar os parâmetros e métricas no MLflow\n",
    "    mlflow.log_param(\"Porcentagem_de_Teste_Dev\", dev_test_ratio)\n",
    "    mlflow.log_param(\"Porcentagem_de_Treino_Dev\", dev_train_ratio)\n",
    "    mlflow.log_param(\"Porcentagem_de_Teste_Prod\", prod_test_ratio)\n",
    "    mlflow.log_param(\"Porcentagem_de_Treino_Prod\", prod_train_ratio)\n",
    "    mlflow.log_metric(\"Tamanho_do_Conjunto_de_Treino_Dev\", len(dev_train))\n",
    "    mlflow.log_metric(\"Tamanho_do_Conjunto_de_Teste_Dev\", len(dev_test))\n",
    "    mlflow.log_metric(\"Tamanho_do_Conjunto_de_Treino_Prod\", len(prod_train))\n",
    "    mlflow.log_metric(\"Tamanho_do_Conjunto_de_Teste_Prod\", len(prod_test))\n",
    "    \n",
    "    # Setup dos dados para treinamento usando PyCaret\n",
    "    setup_params = {\n",
    "        \"target\": \"shot_made_flag\",\n",
    "        \"session_id\": 42\n",
    "    }\n",
    "    \n",
    "    setup(**setup_params)\n",
    "    # Treinar modelos de classificação usando PyCaret\n",
    "    automl_model = automl(optimize=\"AUC\")\n",
    "    \n",
    "    # Salvar o melhor modelo como um arquivo .pkl\n",
    "    save_model(automl_model, \"best_model_pycaret\")\n",
    "    \n",
    "    # Fazer previsões nos dados de teste com o melhor modelo\n",
    "    predictions = predict_model(automl_model, data=dev_test)\n",
    "\n",
    "    # Log das métricas\n",
    "    mlflow.log_metric(\"AUC\", roc_auc_score(predictions['shot_made_flag'], predictions['prediction_label']))\n",
    "    mlflow.log_metric(\"Log Loss\", log_loss(predictions['shot_made_flag'], predictions['prediction_label']))\n",
    "    mlflow.log_metric(\"F1 Score\", f1_score(predictions['shot_made_flag'], predictions['prediction_label']))\n",
    "\n",
    "    # Salvar o modelo no MLflow\n",
    "    mlflow.sklearn.log_model(automl_model, \"best_model\")\n",
    "\n",
    "    # Registrar o modelo para produção\n",
    "    model_uri = f\"file:///C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/models/best_model\"\n",
    "    mlflow.register_model(model_uri, \"best_model_production\")\n",
    "\n",
    "    # Transicionar o modelo para o estágio de produção\n",
    "    client = MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=\"best_model_production\",\n",
    "        version=1, \n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas para os dados de produção:\n",
      "Acurácia: 0.6\n",
      "Precisão: 0.6\n",
      "Recall: 1.0\n",
      "F1-Score: 0.7499999999999999\n",
      "Log Loss: 0.6931471805599453\n",
      "\n",
      "Métricas para os dados de desenvolvimento:\n",
      "Acurácia: 0.6\n",
      "Precisão: 0.6\n",
      "Recall: 1.0\n",
      "F1-Score: 0.7499999999999999\n",
      "Log Loss: 0.8171703953063254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "\n",
    "# Dados de produção\n",
    "predicted_probs_prod = [0.5, 0.5, 0.5, 0.5, 0.5]  # Substitua pelos valores reais\n",
    "real_results_prod = [0, 1, 1, 0, 1]  # Substitua pelos valores reais\n",
    "\n",
    "# Dados de desenvolvimento\n",
    "predicted_probs_dev = [0.5, 0.5, 0.5, 0.731059, 0.5]  # Substitua pelos valores reais\n",
    "real_results_dev = [0, 1, 1, 0, 1]  # Substitua pelos valores reais\n",
    "\n",
    "# Calcular as métricas para os dados de produção\n",
    "accuracy_prod = accuracy_score(real_results_prod, [1 if p >= 0.5 else 0 for p in predicted_probs_prod])\n",
    "precision_prod = precision_score(real_results_prod, [1 if p >= 0.5 else 0 for p in predicted_probs_prod])\n",
    "recall_prod = recall_score(real_results_prod, [1 if p >= 0.5 else 0 for p in predicted_probs_prod])\n",
    "f1_prod = f1_score(real_results_prod, [1 if p >= 0.5 else 0 for p in predicted_probs_prod])\n",
    "logloss_prod = log_loss(real_results_prod, predicted_probs_prod)\n",
    "\n",
    "# Calcular as métricas para os dados de desenvolvimento\n",
    "accuracy_dev = accuracy_score(real_results_dev, [1 if p >= 0.5 else 0 for p in predicted_probs_dev])\n",
    "precision_dev = precision_score(real_results_dev, [1 if p >= 0.5 else 0 for p in predicted_probs_dev])\n",
    "recall_dev = recall_score(real_results_dev, [1 if p >= 0.5 else 0 for p in predicted_probs_dev])\n",
    "f1_dev = f1_score(real_results_dev, [1 if p >= 0.5 else 0 for p in predicted_probs_dev])\n",
    "logloss_dev = log_loss(real_results_dev, predicted_probs_dev)\n",
    "\n",
    "# Imprimir as métricas\n",
    "print(\"Métricas para os dados de produção:\")\n",
    "print(f\"Acurácia: {accuracy_prod}\")\n",
    "print(f\"Precisão: {precision_prod}\")\n",
    "print(f\"Recall: {recall_prod}\")\n",
    "\n",
    "print(f\"F1-Score: {f1_prod}\")\n",
    "print(f\"Log Loss: {logloss_prod}\")\n",
    "print(\"\\nMétricas para os dados de desenvolvimento:\")\n",
    "print(f\"Acurácia: {accuracy_dev}\")\n",
    "print(f\"Precisão: {precision_dev}\")\n",
    "print(f\"Recall: {recall_dev}\")\n",
    "print(f\"F1-Score: {f1_dev}\")\n",
    "print(f\"Log Loss: {logloss_dev}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de produção:\n",
      "   probability\n",
      "0          0.5\n",
      "1          0.5\n",
      "2          0.5\n",
      "3          0.5\n",
      "4          0.5\n",
      "\n",
      "Dados de desenvolvimento:\n",
      "   probability\n",
      "0     0.500000\n",
      "1     0.500000\n",
      "2     0.500000\n",
      "3     0.731059\n",
      "4     0.500000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os dados de produção\n",
    "prod_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/code/predictions_prod_2024-05-09_22-28-49.parquet\"\n",
    "prod_data = pd.read_parquet(prod_data_path)\n",
    "\n",
    "# Carregar os dados de desenvolvimento\n",
    "dev_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/code/predictions_dev_2024-05-09_22-34-50.parquet\"\n",
    "dev_data = pd.read_parquet(dev_data_path)\n",
    "\n",
    "# Visualizar os dados\n",
    "print(\"Dados de produção:\")\n",
    "print(prod_data.head())\n",
    "\n",
    "print(\"\\nDados de desenvolvimento:\")\n",
    "print(dev_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n",
      "Resultados do modelo para os dados de desenvolvimento:\n",
      "Log Loss: 6.7659844798136985\n",
      "F1 Score: 0.5223325750430006\n",
      "\n",
      "Resultados do modelo para os dados de produção:\n",
      "Log Loss: 5.252267767958857\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "from pycaret.classification import load_model\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def load_production_data(file_path):\n",
    "    # Carregar a base de produção\n",
    "    prod_data = pd.read_parquet(file_path)\n",
    "    return prod_data\n",
    "\n",
    "def load_dev_data(file_path):\n",
    "    # Carregar a base de desenvolvimento\n",
    "    dev_data = pd.read_parquet(file_path)\n",
    "    return dev_data\n",
    "\n",
    "def apply_model(model, data):\n",
    "    # Aplicar o modelo aos dados e obter as pontuações\n",
    "    scores = model.predict(data)\n",
    "    # Aplicar função sigmoid para transformar em probabilidades\n",
    "    probabilities = 1 / (1 + np.exp(-scores))\n",
    "    return probabilities\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    # Avaliar o modelo com os dados\n",
    "    y_pred = model.predict(X)\n",
    "    logloss = log_loss(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    return logloss, f1\n",
    "\n",
    "def compare_datasets(dev_data, prod_data):\n",
    "    # Comparar resultados do modelo entre os conjuntos de dados\n",
    "    logloss_dev, f1_dev = evaluate_model(model, dev_data.drop(columns=['shot_made_flag']), dev_data['shot_made_flag'])\n",
    "    logloss_prod, f1_prod = evaluate_model(model, prod_data.drop(columns=['shot_made_flag']), prod_data['shot_made_flag'])\n",
    "\n",
    "    print(\"Resultados do modelo para os dados de desenvolvimento:\")\n",
    "    print(\"Log Loss:\", logloss_dev)\n",
    "    print(\"F1 Score:\", f1_dev)\n",
    "    print()\n",
    "\n",
    "    print(\"Resultados do modelo para os dados de produção:\")\n",
    "    print(\"Log Loss:\", logloss_prod)\n",
    "    print(\"F1 Score:\", f1_prod)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Carregar o modelo treinado\n",
    "    model_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/code/mlruns/216971763806645027/4dbe79efd23f4d4984b0522e3ad0bd46/artifacts/model/model\"\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Carregar os dados de produção\n",
    "    production_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/processed/data_filtered_prod.parquet\"\n",
    "    prod_data = load_production_data(production_data_path)\n",
    "\n",
    "    # Carregar os dados de desenvolvimento\n",
    "    dev_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/processed/data_filtered_dev.parquet\"\n",
    "    dev_data = load_dev_data(dev_data_path)\n",
    "\n",
    "    # Comparar resultados do modelo entre os conjuntos de dados\n",
    "    compare_datasets(dev_data, prod_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "from pycaret.classification import load_model\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def load_production_data(file_path):\n",
    "    # Carregar a base de produção\n",
    "    prod_data = pd.read_parquet(file_path)\n",
    "    return prod_data\n",
    "\n",
    "def load_dev_data(file_path):\n",
    "    # Carregar a base de desenvolvimento\n",
    "    dev_data = pd.read_parquet(file_path)\n",
    "    return dev_data\n",
    "\n",
    "def apply_model(model, data):\n",
    "    # Aplicar o modelo aos dados e obter as pontuações\n",
    "    scores = model.predict(data)\n",
    "    # Aplicar função sigmoid para transformar em probabilidades\n",
    "    probabilities = 1 / (1 + np.exp(-scores))\n",
    "    return probabilities\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    # Avaliar o modelo com os dados\n",
    "    y_pred = model.predict(X)\n",
    "    logloss = log_loss(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    return logloss, f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Carregar o modelo treinado\n",
    "    model_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/code/mlruns/216971763806645027/4dbe79efd23f4d4984b0522e3ad0bd46/artifacts/model/model\"\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Carregar os dados de produção\n",
    "    production_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/processed/data_filtered_prod.parquet\"\n",
    "    prod_data = load_production_data(production_data_path)\n",
    "\n",
    "    # Carregar os dados de desenvolvimento\n",
    "    dev_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/processed/data_filtered_dev.parquet\"\n",
    "    dev_data = load_dev_data(dev_data_path)\n",
    "\n",
    "    # Aplicar o modelo aos dados de produção\n",
    "    predictions_prod = apply_model(model, prod_data.drop(columns=['shot_made_flag']))\n",
    "\n",
    "    # Avaliar o modelo com os dados de produção\n",
    "    logloss_prod, f1_prod = evaluate_model(model, prod_data.drop(columns=['shot_made_flag']), prod_data['shot_made_flag'])\n",
    "\n",
    "    # Aplicar o modelo aos dados de desenvolvimento\n",
    "    predictions_dev = apply_model(model, dev_data.drop(columns=['shot_made_flag']))\n",
    "\n",
    "    # Avaliar o modelo com os dados de desenvolvimento\n",
    "    logloss_dev, f1_dev = evaluate_model(model, dev_data.drop(columns=['shot_made_flag']), dev_data['shot_made_flag'])\n",
    "\n",
    "    # Nomear a rodada do MLflow\n",
    "    mlflow.set_experiment(\"PipelineAplicacao\")\n",
    "    with mlflow.start_run(run_name=\"PipelineAplicacao\"):\n",
    "        # Salvar resultados como artefatos\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        \n",
    "        # Converter os arrays numpy em dataframes do pandas\n",
    "        predictions_prod_df = pd.DataFrame(predictions_prod, columns=['probability'])\n",
    "        predictions_dev_df = pd.DataFrame(predictions_dev, columns=['probability'])\n",
    "\n",
    "        # Salvar os dataframes como arquivos parquet\n",
    "        predictions_prod_df.to_parquet(f\"predictions_prod_{timestamp}.parquet\")\n",
    "        predictions_dev_df.to_parquet(f\"predictions_dev_{timestamp}.parquet\")\n",
    "        \n",
    "        # Logar as métricas de produção\n",
    "        mlflow.log_metric(\"log_loss_prod\", logloss_prod)\n",
    "        mlflow.log_metric(\"f1_score_prod\", f1_prod)\n",
    "\n",
    "        # Logar as métricas de desenvolvimento\n",
    "        mlflow.log_metric(\"log_loss_dev\", logloss_dev)\n",
    "        mlflow.log_metric(\"f1_score_dev\", f1_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A corrida MODELOBOM foi registrada e contém os seguintes artefatos (modelos):\n",
      "file:///c:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/code/mlruns/216971763806645027/4dbe79efd23f4d4984b0522e3ad0bd46/artifacts\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Listar todas as corridas\n",
    "runs = mlflow.search_runs()\n",
    "\n",
    "# Filtrar as corridas pelo nome do experimento\n",
    "runs_modelobom = runs[runs['tags.mlflow.runName'] == \"MODELOBOM\"]\n",
    "\n",
    "if not runs_modelobom.empty:\n",
    "    # Se houver corridas com o nome \"MODELOBOM\", obtemos o ID da primeira corrida\n",
    "    run_id = runs_modelobom.iloc[0]['run_id']\n",
    "    \n",
    "    # Obter informações sobre a corrida MODELOBOM\n",
    "    run_info = mlflow.get_run(run_id)\n",
    "    \n",
    "    # Acessar os artefatos (modelos) diretamente\n",
    "    artifacts = run_info.to_dictionary()['info']['artifact_uri']\n",
    "    \n",
    "    if artifacts:\n",
    "        print(\"A corrida MODELOBOM foi registrada e contém os seguintes artefatos (modelos):\")\n",
    "        print(artifacts)\n",
    "    else:\n",
    "        print(\"A corrida MODELOBOM foi registrada, mas não contém modelos.\")\n",
    "else:\n",
    "    print(\"A corrida MODELOBOM não foi registrada no MLflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/09 21:22:12 INFO mlflow.tracking.fluent: Experiment with name 'Treinamento' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas para Regressão Logística:\n",
      "Log Loss: 0.6783615727270217\n",
      "Métricas para Árvore de Decisão:\n",
      "Log Loss: 4.3672161877959015\n",
      "F1 Score: 0.8137086903304773\n",
      "O melhor modelo é: Regressão Logística\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "from pycaret.classification import setup, create_model\n",
    "\n",
    "#  MLflow\n",
    "mlflow.set_experiment(\"Treinamento\")\n",
    "\n",
    "#  treinamento e teste filtrados\n",
    "dev_processed_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/processed/data_filtered_dev.parquet\"\n",
    "prod_processed_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/processed/data_filtered_prod.parquet\"\n",
    "dev_df = pd.read_parquet(dev_processed_path)\n",
    "prod_df = pd.read_parquet(prod_processed_path)\n",
    "\n",
    "# Separar os dados filtrados para treinamento e teste\n",
    "X_dev = dev_df.drop(columns=['shot_made_flag'])\n",
    "y_dev = dev_df['shot_made_flag']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.2, stratify=y_dev, random_state=42)\n",
    "\n",
    "# Verificar se uma corrida está ativa\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Iniciar uma nova corrida do MLflow\n",
    "with mlflow.start_run(run_name=\"Treinamento\"):\n",
    "\n",
    "    # Treinar o modelo de regressão logística\n",
    "    with mlflow.start_run(run_name=\"Logistic Regression\", nested=True):\n",
    "        lr_model = create_model('lr', verbose=False)\n",
    "        lr_pred_test = lr_model.predict_proba(X_test)\n",
    "        lr_logloss = log_loss(y_test, lr_pred_test)\n",
    "        mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "        mlflow.log_metric(\"log_loss\", lr_logloss)\n",
    "        print(f\"Métricas para Regressão Logística:\\nLog Loss: {lr_logloss}\")\n",
    "\n",
    "    #  árvore de decisão\n",
    "    with mlflow.start_run(run_name=\"Decision Tree\", nested=True):\n",
    "        tree_model = create_model('dt', verbose=False)\n",
    "        tree_pred_test = tree_model.predict_proba(X_test)\n",
    "        tree_logloss = log_loss(y_test, tree_pred_test)\n",
    "        tree_f1 = f1_score(y_test, tree_model.predict(X_test))\n",
    "        mlflow.log_param(\"model\", \"Decision Tree\")\n",
    "        mlflow.log_metric(\"log_loss\", tree_logloss)\n",
    "        mlflow.log_metric(\"f1_score\", tree_f1)\n",
    "        print(f\"Métricas para Árvore de Decisão:\\nLog Loss: {tree_logloss}\\nF1 Score: {tree_f1}\")\n",
    "\n",
    "    # Selecionar o modelo com menor log loss para finalização\n",
    "    if lr_logloss < tree_logloss:\n",
    "        best_model = lr_model\n",
    "        best_model_name = \"Regressão Logística\"\n",
    "    else:\n",
    "        best_model = tree_model\n",
    "        best_model_name = \"Árvore de Decisão\"\n",
    "\n",
    "    print(f\"O melhor modelo é: {best_model_name}\")\n",
    "\n",
    "    # Encerrar a corrida atual do MLflow\n",
    "    mlflow.end_run()\n",
    "\n",
    "    # Iniciar uma nova corrida para salvar o melhor modelo\n",
    "    with mlflow.start_run(run_name=\"Melhor Modelo\"):\n",
    "\n",
    "        # Log do modelo\n",
    "        mlflow.sklearn.log_model(best_model, \"model\")\n",
    "\n",
    "        # Log dos parâmetros do modelo\n",
    "        mlflow.log_param(\"model_name\", best_model_name)\n",
    "\n",
    "        # Log das métricas\n",
    "        mlflow.log_metric(\"log_loss\", min(lr_logloss, tree_logloss))\n",
    "        if best_model_name == \"Árvore de Decisão\":\n",
    "            mlflow.log_metric(\"f1_score\", tree_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas presentes nos dados de treinamento:\n",
      "Index(['lat', 'lon', 'minutes_remaining', 'period', 'playoffs',\n",
      "       'shot_distance'],\n",
      "      dtype='object')\n",
      "Colunas presentes nos dados de teste:\n",
      "Index(['lat', 'lon', 'minutes_remaining', 'period', 'playoffs',\n",
      "       'shot_distance'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_59c23_row8_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_59c23\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_59c23_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_59c23_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_59c23_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_59c23_row0_col1\" class=\"data row0 col1\" >241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_59c23_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_59c23_row1_col1\" class=\"data row1 col1\" >shot_made_flag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_59c23_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_59c23_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_59c23_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_59c23_row3_col1\" class=\"data row3 col1\" >(20285, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_59c23_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_59c23_row4_col1\" class=\"data row4 col1\" >(20285, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_59c23_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_59c23_row5_col1\" class=\"data row5 col1\" >(14199, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_59c23_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_59c23_row6_col1\" class=\"data row6 col1\" >(6086, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_59c23_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_59c23_row7_col1\" class=\"data row7 col1\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_59c23_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
       "      <td id=\"T_59c23_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_59c23_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
       "      <td id=\"T_59c23_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_59c23_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_59c23_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_59c23_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_59c23_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_59c23_row12_col0\" class=\"data row12 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_59c23_row12_col1\" class=\"data row12 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_59c23_row13_col0\" class=\"data row13 col0\" >Fold Number</td>\n",
       "      <td id=\"T_59c23_row13_col1\" class=\"data row13 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_59c23_row14_col0\" class=\"data row14 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_59c23_row14_col1\" class=\"data row14 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_59c23_row15_col0\" class=\"data row15 col0\" >Use GPU</td>\n",
       "      <td id=\"T_59c23_row15_col1\" class=\"data row15 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_59c23_row16_col0\" class=\"data row16 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_59c23_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_59c23_row17_col0\" class=\"data row17 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_59c23_row17_col1\" class=\"data row17 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_59c23_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_59c23_row18_col0\" class=\"data row18 col0\" >USI</td>\n",
       "      <td id=\"T_59c23_row18_col1\" class=\"data row18 col1\" >dcce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2972071be50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.classification import setup\n",
    "import mlflow\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "#  MLflow\n",
    "mlflow.set_experiment(\"PreparacaoDados\")\n",
    "\n",
    "# Iniciar novo  MLflow\n",
    "with mlflow.start_run(run_name=\"PreparacaoDados\"):\n",
    "\n",
    "    # C treinamento e teste\n",
    "    dev_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/raw/dataset_kobe_dev.parquet\"\n",
    "    prod_data_path = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/raw/dataset_kobe_prod.parquet\"\n",
    "    dev_df = pd.read_parquet(dev_data_path)\n",
    "    prod_df = pd.read_parquet(prod_data_path)\n",
    "\n",
    "    # colunas necessárias\n",
    "    selected_columns = ['lat', 'lon', 'minutes_remaining', 'period', 'playoffs', 'shot_distance', 'shot_made_flag']\n",
    "    dev_df = dev_df[selected_columns]\n",
    "    prod_df = prod_df[selected_columns]\n",
    "\n",
    "    # Remover linhas com dados faltantes\n",
    "    dev_df.dropna(inplace=True)\n",
    "    prod_df.dropna(inplace=True)\n",
    "\n",
    "    # Salvar o dataset resultante\n",
    "    processed_data_dir = \"C:/Users/Anderson/Desktop/REDES_NEURAIS/ENGENHARIA_V2/ENGENHARIA_MACHINE_LEARNING_INFNET/data/processed\"\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "    dev_processed_path = os.path.join(processed_data_dir, \"data_filtered_dev.parquet\")\n",
    "    prod_processed_path = os.path.join(processed_data_dir, \"data_filtered_prod.parquet\")\n",
    "    dev_df.to_parquet(dev_processed_path)\n",
    "    prod_df.to_parquet(prod_processed_path)\n",
    "\n",
    "    # Separar os dados em treino e teste (80% treino, 20% teste)\n",
    "    X_dev = dev_df.drop(columns=['shot_made_flag'])\n",
    "    y_dev = dev_df['shot_made_flag']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.2, stratify=y_dev, random_state=42)\n",
    "\n",
    "    # Salvar os datasets de treino e teste\n",
    "    train_data_path = os.path.join(processed_data_dir, \"base_train.parquet\")\n",
    "    test_data_path = os.path.join(processed_data_dir, \"base_test.parquet\")\n",
    "    X_train.to_parquet(train_data_path)\n",
    "    y_train.to_frame().to_parquet(train_data_path.replace('.parquet', '_target.parquet'))\n",
    "    X_test.to_parquet(test_data_path)\n",
    "    y_test.to_frame().to_parquet(test_data_path.replace('.parquet', '_target.parquet'))\n",
    "\n",
    "    # Verificar se a coluna shot_made_flag está presente nos dados de treinamento e teste\n",
    "    print(\"Colunas presentes nos dados de treinamento:\")\n",
    "    print(X_train.columns)\n",
    "    print(\"Colunas presentes nos dados de teste:\")\n",
    "    print(X_test.columns)\n",
    "\n",
    "    # Configurar o ambiente de classificação do PyCaret\n",
    "    clf_setup = setup(data=dev_df, target='shot_made_flag')\n",
    "\n",
    "    # Registar os parâmetros no MLflow\n",
    "    mlflow.log_param(\"test_size_percent\", 0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "234250f21dfd4e4ef32f5f4db8c2e934a9d2d678166812e687c6eaac9839cad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
